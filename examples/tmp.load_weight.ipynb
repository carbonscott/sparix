{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot       as plt\n",
    "import matplotlib.colors       as mcolors\n",
    "import matplotlib.patches      as mpatches\n",
    "import matplotlib.transforms   as mtransforms\n",
    "import matplotlib.font_manager as font_manager\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poorman_transformer.modeling.transformer import Transformer, TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparix.trans import Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_weight = \"chkpts/google.vit-base-patch16-224-in21k\"\n",
    "model_weight = torch.load(path_model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_dict = model_weight.get('model_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token\n",
      "vit.embeddings.position_embeddings\n",
      "vit.embeddings.patch_embeddings.projection.weight\n",
      "vit.embeddings.patch_embeddings.projection.bias\n",
      "vit.encoder.layer.0.attention.attention.query.weight\n",
      "vit.encoder.layer.0.attention.attention.query.bias\n",
      "vit.encoder.layer.0.attention.attention.key.weight\n",
      "vit.encoder.layer.0.attention.attention.key.bias\n",
      "vit.encoder.layer.0.attention.attention.value.weight\n",
      "vit.encoder.layer.0.attention.attention.value.bias\n",
      "vit.encoder.layer.0.attention.output.dense.weight\n",
      "vit.encoder.layer.0.attention.output.dense.bias\n",
      "vit.encoder.layer.0.intermediate.dense.weight\n",
      "vit.encoder.layer.0.intermediate.dense.bias\n",
      "vit.encoder.layer.0.output.dense.weight\n",
      "vit.encoder.layer.0.output.dense.bias\n",
      "vit.encoder.layer.0.layernorm_before.weight\n",
      "vit.encoder.layer.0.layernorm_before.bias\n",
      "vit.encoder.layer.0.layernorm_after.weight\n",
      "vit.encoder.layer.0.layernorm_after.bias\n",
      "vit.encoder.layer.1.attention.attention.query.weight\n",
      "vit.encoder.layer.1.attention.attention.query.bias\n",
      "vit.encoder.layer.1.attention.attention.key.weight\n",
      "vit.encoder.layer.1.attention.attention.key.bias\n",
      "vit.encoder.layer.1.attention.attention.value.weight\n",
      "vit.encoder.layer.1.attention.attention.value.bias\n",
      "vit.encoder.layer.1.attention.output.dense.weight\n",
      "vit.encoder.layer.1.attention.output.dense.bias\n",
      "vit.encoder.layer.1.intermediate.dense.weight\n",
      "vit.encoder.layer.1.intermediate.dense.bias\n",
      "vit.encoder.layer.1.output.dense.weight\n",
      "vit.encoder.layer.1.output.dense.bias\n",
      "vit.encoder.layer.1.layernorm_before.weight\n",
      "vit.encoder.layer.1.layernorm_before.bias\n",
      "vit.encoder.layer.1.layernorm_after.weight\n",
      "vit.encoder.layer.1.layernorm_after.bias\n",
      "vit.encoder.layer.2.attention.attention.query.weight\n",
      "vit.encoder.layer.2.attention.attention.query.bias\n",
      "vit.encoder.layer.2.attention.attention.key.weight\n",
      "vit.encoder.layer.2.attention.attention.key.bias\n",
      "vit.encoder.layer.2.attention.attention.value.weight\n",
      "vit.encoder.layer.2.attention.attention.value.bias\n",
      "vit.encoder.layer.2.attention.output.dense.weight\n",
      "vit.encoder.layer.2.attention.output.dense.bias\n",
      "vit.encoder.layer.2.intermediate.dense.weight\n",
      "vit.encoder.layer.2.intermediate.dense.bias\n",
      "vit.encoder.layer.2.output.dense.weight\n",
      "vit.encoder.layer.2.output.dense.bias\n",
      "vit.encoder.layer.2.layernorm_before.weight\n",
      "vit.encoder.layer.2.layernorm_before.bias\n",
      "vit.encoder.layer.2.layernorm_after.weight\n",
      "vit.encoder.layer.2.layernorm_after.bias\n",
      "vit.encoder.layer.3.attention.attention.query.weight\n",
      "vit.encoder.layer.3.attention.attention.query.bias\n",
      "vit.encoder.layer.3.attention.attention.key.weight\n",
      "vit.encoder.layer.3.attention.attention.key.bias\n",
      "vit.encoder.layer.3.attention.attention.value.weight\n",
      "vit.encoder.layer.3.attention.attention.value.bias\n",
      "vit.encoder.layer.3.attention.output.dense.weight\n",
      "vit.encoder.layer.3.attention.output.dense.bias\n",
      "vit.encoder.layer.3.intermediate.dense.weight\n",
      "vit.encoder.layer.3.intermediate.dense.bias\n",
      "vit.encoder.layer.3.output.dense.weight\n",
      "vit.encoder.layer.3.output.dense.bias\n",
      "vit.encoder.layer.3.layernorm_before.weight\n",
      "vit.encoder.layer.3.layernorm_before.bias\n",
      "vit.encoder.layer.3.layernorm_after.weight\n",
      "vit.encoder.layer.3.layernorm_after.bias\n",
      "vit.encoder.layer.4.attention.attention.query.weight\n",
      "vit.encoder.layer.4.attention.attention.query.bias\n",
      "vit.encoder.layer.4.attention.attention.key.weight\n",
      "vit.encoder.layer.4.attention.attention.key.bias\n",
      "vit.encoder.layer.4.attention.attention.value.weight\n",
      "vit.encoder.layer.4.attention.attention.value.bias\n",
      "vit.encoder.layer.4.attention.output.dense.weight\n",
      "vit.encoder.layer.4.attention.output.dense.bias\n",
      "vit.encoder.layer.4.intermediate.dense.weight\n",
      "vit.encoder.layer.4.intermediate.dense.bias\n",
      "vit.encoder.layer.4.output.dense.weight\n",
      "vit.encoder.layer.4.output.dense.bias\n",
      "vit.encoder.layer.4.layernorm_before.weight\n",
      "vit.encoder.layer.4.layernorm_before.bias\n",
      "vit.encoder.layer.4.layernorm_after.weight\n",
      "vit.encoder.layer.4.layernorm_after.bias\n",
      "vit.encoder.layer.5.attention.attention.query.weight\n",
      "vit.encoder.layer.5.attention.attention.query.bias\n",
      "vit.encoder.layer.5.attention.attention.key.weight\n",
      "vit.encoder.layer.5.attention.attention.key.bias\n",
      "vit.encoder.layer.5.attention.attention.value.weight\n",
      "vit.encoder.layer.5.attention.attention.value.bias\n",
      "vit.encoder.layer.5.attention.output.dense.weight\n",
      "vit.encoder.layer.5.attention.output.dense.bias\n",
      "vit.encoder.layer.5.intermediate.dense.weight\n",
      "vit.encoder.layer.5.intermediate.dense.bias\n",
      "vit.encoder.layer.5.output.dense.weight\n",
      "vit.encoder.layer.5.output.dense.bias\n",
      "vit.encoder.layer.5.layernorm_before.weight\n",
      "vit.encoder.layer.5.layernorm_before.bias\n",
      "vit.encoder.layer.5.layernorm_after.weight\n",
      "vit.encoder.layer.5.layernorm_after.bias\n",
      "vit.encoder.layer.6.attention.attention.query.weight\n",
      "vit.encoder.layer.6.attention.attention.query.bias\n",
      "vit.encoder.layer.6.attention.attention.key.weight\n",
      "vit.encoder.layer.6.attention.attention.key.bias\n",
      "vit.encoder.layer.6.attention.attention.value.weight\n",
      "vit.encoder.layer.6.attention.attention.value.bias\n",
      "vit.encoder.layer.6.attention.output.dense.weight\n",
      "vit.encoder.layer.6.attention.output.dense.bias\n",
      "vit.encoder.layer.6.intermediate.dense.weight\n",
      "vit.encoder.layer.6.intermediate.dense.bias\n",
      "vit.encoder.layer.6.output.dense.weight\n",
      "vit.encoder.layer.6.output.dense.bias\n",
      "vit.encoder.layer.6.layernorm_before.weight\n",
      "vit.encoder.layer.6.layernorm_before.bias\n",
      "vit.encoder.layer.6.layernorm_after.weight\n",
      "vit.encoder.layer.6.layernorm_after.bias\n",
      "vit.encoder.layer.7.attention.attention.query.weight\n",
      "vit.encoder.layer.7.attention.attention.query.bias\n",
      "vit.encoder.layer.7.attention.attention.key.weight\n",
      "vit.encoder.layer.7.attention.attention.key.bias\n",
      "vit.encoder.layer.7.attention.attention.value.weight\n",
      "vit.encoder.layer.7.attention.attention.value.bias\n",
      "vit.encoder.layer.7.attention.output.dense.weight\n",
      "vit.encoder.layer.7.attention.output.dense.bias\n",
      "vit.encoder.layer.7.intermediate.dense.weight\n",
      "vit.encoder.layer.7.intermediate.dense.bias\n",
      "vit.encoder.layer.7.output.dense.weight\n",
      "vit.encoder.layer.7.output.dense.bias\n",
      "vit.encoder.layer.7.layernorm_before.weight\n",
      "vit.encoder.layer.7.layernorm_before.bias\n",
      "vit.encoder.layer.7.layernorm_after.weight\n",
      "vit.encoder.layer.7.layernorm_after.bias\n",
      "vit.encoder.layer.8.attention.attention.query.weight\n",
      "vit.encoder.layer.8.attention.attention.query.bias\n",
      "vit.encoder.layer.8.attention.attention.key.weight\n",
      "vit.encoder.layer.8.attention.attention.key.bias\n",
      "vit.encoder.layer.8.attention.attention.value.weight\n",
      "vit.encoder.layer.8.attention.attention.value.bias\n",
      "vit.encoder.layer.8.attention.output.dense.weight\n",
      "vit.encoder.layer.8.attention.output.dense.bias\n",
      "vit.encoder.layer.8.intermediate.dense.weight\n",
      "vit.encoder.layer.8.intermediate.dense.bias\n",
      "vit.encoder.layer.8.output.dense.weight\n",
      "vit.encoder.layer.8.output.dense.bias\n",
      "vit.encoder.layer.8.layernorm_before.weight\n",
      "vit.encoder.layer.8.layernorm_before.bias\n",
      "vit.encoder.layer.8.layernorm_after.weight\n",
      "vit.encoder.layer.8.layernorm_after.bias\n",
      "vit.encoder.layer.9.attention.attention.query.weight\n",
      "vit.encoder.layer.9.attention.attention.query.bias\n",
      "vit.encoder.layer.9.attention.attention.key.weight\n",
      "vit.encoder.layer.9.attention.attention.key.bias\n",
      "vit.encoder.layer.9.attention.attention.value.weight\n",
      "vit.encoder.layer.9.attention.attention.value.bias\n",
      "vit.encoder.layer.9.attention.output.dense.weight\n",
      "vit.encoder.layer.9.attention.output.dense.bias\n",
      "vit.encoder.layer.9.intermediate.dense.weight\n",
      "vit.encoder.layer.9.intermediate.dense.bias\n",
      "vit.encoder.layer.9.output.dense.weight\n",
      "vit.encoder.layer.9.output.dense.bias\n",
      "vit.encoder.layer.9.layernorm_before.weight\n",
      "vit.encoder.layer.9.layernorm_before.bias\n",
      "vit.encoder.layer.9.layernorm_after.weight\n",
      "vit.encoder.layer.9.layernorm_after.bias\n",
      "vit.encoder.layer.10.attention.attention.query.weight\n",
      "vit.encoder.layer.10.attention.attention.query.bias\n",
      "vit.encoder.layer.10.attention.attention.key.weight\n",
      "vit.encoder.layer.10.attention.attention.key.bias\n",
      "vit.encoder.layer.10.attention.attention.value.weight\n",
      "vit.encoder.layer.10.attention.attention.value.bias\n",
      "vit.encoder.layer.10.attention.output.dense.weight\n",
      "vit.encoder.layer.10.attention.output.dense.bias\n",
      "vit.encoder.layer.10.intermediate.dense.weight\n",
      "vit.encoder.layer.10.intermediate.dense.bias\n",
      "vit.encoder.layer.10.output.dense.weight\n",
      "vit.encoder.layer.10.output.dense.bias\n",
      "vit.encoder.layer.10.layernorm_before.weight\n",
      "vit.encoder.layer.10.layernorm_before.bias\n",
      "vit.encoder.layer.10.layernorm_after.weight\n",
      "vit.encoder.layer.10.layernorm_after.bias\n",
      "vit.encoder.layer.11.attention.attention.query.weight\n",
      "vit.encoder.layer.11.attention.attention.query.bias\n",
      "vit.encoder.layer.11.attention.attention.key.weight\n",
      "vit.encoder.layer.11.attention.attention.key.bias\n",
      "vit.encoder.layer.11.attention.attention.value.weight\n",
      "vit.encoder.layer.11.attention.attention.value.bias\n",
      "vit.encoder.layer.11.attention.output.dense.weight\n",
      "vit.encoder.layer.11.attention.output.dense.bias\n",
      "vit.encoder.layer.11.intermediate.dense.weight\n",
      "vit.encoder.layer.11.intermediate.dense.bias\n",
      "vit.encoder.layer.11.output.dense.weight\n",
      "vit.encoder.layer.11.output.dense.bias\n",
      "vit.encoder.layer.11.layernorm_before.weight\n",
      "vit.encoder.layer.11.layernorm_before.bias\n",
      "vit.encoder.layer.11.layernorm_after.weight\n",
      "vit.encoder.layer.11.layernorm_after.bias\n",
      "vit.layernorm.weight\n",
      "vit.layernorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model_weight.get('model_state_dict').keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight_dict[\"vit.embeddings.position_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lib_size = 56\n",
    "embd_size = 768\n",
    "context_length = 32\n",
    "num_blocks = 1\n",
    "num_heads  = 1\n",
    "encoder = TransformerBlock(\n",
    "                      embd_size,\n",
    "                      context_length,\n",
    "                      num_heads,\n",
    "                      uses_causal_mask  = False,\n",
    "                      attention_dropout = 0.0,\n",
    "                      residual_dropout  = 0.0,\n",
    "                      feedforward_dropout = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lib_size = 56\n",
    "embd_size = 768\n",
    "context_length = 32\n",
    "num_blocks = 1\n",
    "num_heads  = 1\n",
    "encoder = Transformer(token_lib_size,\n",
    "                      embd_size,\n",
    "                      context_length,\n",
    "                      num_blocks,\n",
    "                      num_heads,\n",
    "                      uses_causal_mask  = False,\n",
    "                      attention_dropout = 0.0,\n",
    "                      residual_dropout  = 0.0,\n",
    "                      feedforward_dropout = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_head_att_layer.mask\n",
      "multi_head_att_layer.proj_q.weight\n",
      "multi_head_att_layer.proj_q.bias\n",
      "multi_head_att_layer.proj_k.weight\n",
      "multi_head_att_layer.proj_k.bias\n",
      "multi_head_att_layer.proj_v.weight\n",
      "multi_head_att_layer.proj_v.bias\n",
      "multi_head_att_layer.proj_linear.weight\n",
      "multi_head_att_layer.proj_linear.bias\n",
      "ff_layer.ff_layer.0.weight\n",
      "ff_layer.ff_layer.0.bias\n",
      "ff_layer.ff_layer.2.weight\n",
      "ff_layer.ff_layer.2.bias\n",
      "layer_norm_pre_multi_head.weight\n",
      "layer_norm_pre_multi_head.bias\n",
      "layer_norm_pre_feedforward.weight\n",
      "layer_norm_pre_feedforward.bias\n"
     ]
    }
   ],
   "source": [
    "encoder_state_dict = encoder.state_dict()\n",
    "for key in encoder_state_dict.keys(): print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_to_custom_dict = {\n",
    "    \"multi_head_att_layer.proj_q.weight\"      : \"vit.encoder.layer.0.attention.attention.query.weight\",\n",
    "    \"multi_head_att_layer.proj_q.bias\"        : \"vit.encoder.layer.0.attention.attention.query.bias\",\n",
    "    \"multi_head_att_layer.proj_k.weight\"      : \"vit.encoder.layer.0.attention.attention.key.weight\",\n",
    "    \"multi_head_att_layer.proj_k.bias\"        : \"vit.encoder.layer.0.attention.attention.key.bias\",\n",
    "    \"multi_head_att_layer.proj_v.weight\"      : \"vit.encoder.layer.0.attention.attention.value.weight\",\n",
    "    \"multi_head_att_layer.proj_v.bias\"        : \"vit.encoder.layer.0.attention.attention.value.bias\",\n",
    "    \"multi_head_att_layer.proj_linear.weight\" : \"vit.encoder.layer.0.attention.output.dense.weight\",\n",
    "    \"multi_head_att_layer.proj_linear.bias\"   : \"vit.encoder.layer.0.attention.output.dense.bias\",\n",
    "    \"ff_layer.ff_layer.0.weight\"              : \"vit.encoder.layer.0.intermediate.dense.weight\",\n",
    "    \"ff_layer.ff_layer.0.bias\"                : \"vit.encoder.layer.0.intermediate.dense.bias\",\n",
    "    \"ff_layer.ff_layer.2.weight\"              : \"vit.encoder.layer.0.output.dense.weight\",\n",
    "    \"ff_layer.ff_layer.2.bias\"                : \"vit.encoder.layer.0.output.dense.bias\",\n",
    "    \"layer_norm_pre_multi_head.weight\"        : \"vit.encoder.layer.0.layernorm_before.weight\",\n",
    "    \"layer_norm_pre_multi_head.bias\"          : \"vit.encoder.layer.0.layernorm_before.bias\",\n",
    "    \"layer_norm_pre_feedforward.weight\"       : \"vit.encoder.layer.0.layernorm_after.weight\",\n",
    "    \"layer_norm_pre_feedforward.bias\"         : \"vit.encoder.layer.0.layernorm_after.bias\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_to_custom_dict = {\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_q.weight\"      : \"vit.encoder.layer.0.attention.attention.query.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_q.bias\"        : \"vit.encoder.layer.0.attention.attention.query.bias\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_k.weight\"      : \"vit.encoder.layer.0.attention.attention.key.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_k.bias\"        : \"vit.encoder.layer.0.attention.attention.key.bias\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_v.weight\"      : \"vit.encoder.layer.0.attention.attention.value.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_v.bias\"        : \"vit.encoder.layer.0.attention.attention.value.bias\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_linear.weight\" : \"vit.encoder.layer.0.attention.output.dense.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_linear.bias\"   : \"vit.encoder.layer.0.attention.output.dense.bias\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.0.weight\"              : \"vit.encoder.layer.0.intermediate.dense.weight\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.0.bias\"                : \"vit.encoder.layer.0.intermediate.dense.bias\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.2.weight\"              : \"vit.encoder.layer.0.output.dense.weight\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.2.bias\"                : \"vit.encoder.layer.0.output.dense.bias\",\n",
    "    \"transformer_block.0.layer_norm_pre_multi_head.weight\"        : \"vit.encoder.layer.0.layernorm_before.weight\",\n",
    "    \"transformer_block.0.layer_norm_pre_multi_head.bias\"          : \"vit.encoder.layer.0.layernorm_before.bias\",\n",
    "    \"transformer_block.0.layer_norm_pre_feedforward.weight\"       : \"vit.encoder.layer.0.layernorm_after.weight\",\n",
    "    \"transformer_block.0.layer_norm_pre_feedforward.bias\"         : \"vit.encoder.layer.0.layernorm_after.bias\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in encoder_state_dict.keys():\n",
    "    if not k in google_to_custom_dict: continue\n",
    "    k_google = google_to_custom_dict[k]\n",
    "    encoder_state_dict[k] = model_weight_dict[k_google]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (multi_head_att_layer): MultiHeadAttention(\n",
       "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ff_layer): FeedForward(\n",
       "    (ff_layer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (3): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm_pre_multi_head): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm_pre_feedforward): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx, Cx, Hx, Wx = 5000, 1, 129, 129\n",
    "video_clip = torch.randn(Tx, Cx, Hx, Wx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embd_layer = nn.Conv2d(1, embd_size, kernel_size = (16, 16), stride = (16, 16)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 768, 8, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embd = patch_embd_layer(video_clip)\n",
    "patch_embd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_h5 = \"3IYF.Fibonacci.h5\"\n",
    "with h5py.File(path_h5, \"r\") as fh:\n",
    "    data = fh.get(\"intensities\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_patch = 16\n",
    "W_patch = 16\n",
    "H_padded = math.ceil(H / H_patch) * H_patch\n",
    "W_padded = math.ceil(W / W_patch) * W_patch\n",
    "H_padded, W_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = Pad(H_padded, W_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 160, 160)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padded = pad(data.reshape(B * C, H, W)).reshape(B, C, H_padded, W_padded)\n",
    "data_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embd_layer = nn.Conv2d(1, embd_size, kernel_size = (16, 16), stride = (16, 16)).to(device)\n",
    "patch_embd_layer_state_dict = patch_embd_layer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_to_embd_dict = {\n",
    "    \"weight\" : \"vit.embeddings.patch_embeddings.projection.weight\",\n",
    "    \"bias\"   : \"vit.embeddings.patch_embeddings.projection.bias\",\n",
    "}\n",
    "\n",
    "for k in patch_embd_layer_state_dict.keys():\n",
    "    if not k in google_to_embd_dict: continue\n",
    "    k_google = google_to_embd_dict[k]\n",
    "    if model_weight_dict[k_google].ndim == 4:\n",
    "        patch_embd_layer_state_dict[k] = model_weight_dict[k_google].mean(dim = 1, keepdims = True)    # (B, C, H, W), mean along the channel dimension\n",
    "    if model_weight_dict[k_google].ndim == 1:\n",
    "        patch_embd_layer_state_dict[k] = model_weight_dict[k_google]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embd_layer.load_state_dict(patch_embd_layer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768, 10, 10])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embd = patch_embd_layer(torch.tensor(data_padded).to(device))\n",
    "patch_embd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, E, Hp, Wp = patch_embd.shape\n",
    "patch_embd = patch_embd.view(B, E, Hp * Wp).transpose(1, 2).contiguous()    # (B, Hp * Wp, E)\n",
    "patch_embd = patch_embd.view(1, B * Hp * Wp, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peaknet-1.0",
   "language": "python",
   "name": "peaknet-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
