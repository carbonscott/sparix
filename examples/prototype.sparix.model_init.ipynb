{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparix.trans import Pad\n",
    "from sparix.modeling.transformer import Transformer\n",
    "from sparix.data  import FrameDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_weight = \"chkpts/google.vit-base-patch16-224-in21k\"\n",
    "model_weight = torch.load(path_model_weight)\n",
    "model_weight_dict = model_weight.get('model_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embd_layer): Linear(in_features=256, out_features=768, bias=True)\n",
       "  (pos_embd_layer): Embedding(1600, 768)\n",
       "  (transformer_block): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multi_head_att_layer): MultiHeadAttention(\n",
       "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (ff_layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_pre_multi_head): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_pre_feedforward): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multi_head_att_layer): MultiHeadAttention(\n",
       "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (ff_layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_pre_multi_head): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_pre_feedforward): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (multi_head_att_layer): MultiHeadAttention(\n",
       "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (ff_layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_pre_multi_head): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_pre_feedforward): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (multi_head_att_layer): MultiHeadAttention(\n",
       "        (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff_layer): FeedForward(\n",
       "        (ff_layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_pre_multi_head): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_pre_feedforward): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (pred_head): Linear(in_features=768, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hp, Wp = 16, 16\n",
    "num_frame_in_context = 4\n",
    "num_patch = 400\n",
    "\n",
    "tok_size            = Hp * Wp\n",
    "embd_size           = 768    # (google's pretrained ViT)\n",
    "context_length      = num_frame_in_context * num_patch\n",
    "num_blocks          = 4\n",
    "num_heads           = 4\n",
    "uses_causal_mask    = True\n",
    "attention_dropout   = 0.1\n",
    "residual_dropout    = 0.1\n",
    "feedforward_dropout = 0.1\n",
    "# model = Transformer(Hp                  = Hp,\n",
    "#                     Wp                  = Wp,\n",
    "model = Transformer(tok_size            = tok_size,\n",
    "                    embd_size           = embd_size,\n",
    "                    context_length      = context_length,\n",
    "                    num_blocks          = num_blocks,\n",
    "                    num_heads           = num_heads,\n",
    "                    uses_causal_mask    = uses_causal_mask,\n",
    "                    attention_dropout   = attention_dropout,\n",
    "                    residual_dropout    = residual_dropout,\n",
    "                    feedforward_dropout = feedforward_dropout,)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_indices\n",
      "tok_embd_layer.weight\n",
      "tok_embd_layer.bias\n",
      "pos_embd_layer.weight\n",
      "transformer_block.0.multi_head_att_layer.mask\n",
      "transformer_block.0.multi_head_att_layer.proj_q.weight\n",
      "transformer_block.0.multi_head_att_layer.proj_q.bias\n",
      "transformer_block.0.multi_head_att_layer.proj_k.weight\n",
      "transformer_block.0.multi_head_att_layer.proj_k.bias\n",
      "transformer_block.0.multi_head_att_layer.proj_v.weight\n",
      "transformer_block.0.multi_head_att_layer.proj_v.bias\n",
      "transformer_block.0.multi_head_att_layer.proj_linear.weight\n",
      "transformer_block.0.multi_head_att_layer.proj_linear.bias\n",
      "transformer_block.0.ff_layer.ff_layer.0.weight\n",
      "transformer_block.0.ff_layer.ff_layer.0.bias\n",
      "transformer_block.0.ff_layer.ff_layer.2.weight\n",
      "transformer_block.0.ff_layer.ff_layer.2.bias\n",
      "transformer_block.0.layer_norm_pre_multi_head.weight\n",
      "transformer_block.0.layer_norm_pre_multi_head.bias\n",
      "transformer_block.0.layer_norm_pre_feedforward.weight\n",
      "transformer_block.0.layer_norm_pre_feedforward.bias\n",
      "transformer_block.1.multi_head_att_layer.mask\n",
      "transformer_block.1.multi_head_att_layer.proj_q.weight\n",
      "transformer_block.1.multi_head_att_layer.proj_q.bias\n",
      "transformer_block.1.multi_head_att_layer.proj_k.weight\n",
      "transformer_block.1.multi_head_att_layer.proj_k.bias\n",
      "transformer_block.1.multi_head_att_layer.proj_v.weight\n",
      "transformer_block.1.multi_head_att_layer.proj_v.bias\n",
      "transformer_block.1.multi_head_att_layer.proj_linear.weight\n",
      "transformer_block.1.multi_head_att_layer.proj_linear.bias\n",
      "transformer_block.1.ff_layer.ff_layer.0.weight\n",
      "transformer_block.1.ff_layer.ff_layer.0.bias\n",
      "transformer_block.1.ff_layer.ff_layer.2.weight\n",
      "transformer_block.1.ff_layer.ff_layer.2.bias\n",
      "transformer_block.1.layer_norm_pre_multi_head.weight\n",
      "transformer_block.1.layer_norm_pre_multi_head.bias\n",
      "transformer_block.1.layer_norm_pre_feedforward.weight\n",
      "transformer_block.1.layer_norm_pre_feedforward.bias\n",
      "transformer_block.2.multi_head_att_layer.mask\n",
      "transformer_block.2.multi_head_att_layer.proj_q.weight\n",
      "transformer_block.2.multi_head_att_layer.proj_q.bias\n",
      "transformer_block.2.multi_head_att_layer.proj_k.weight\n",
      "transformer_block.2.multi_head_att_layer.proj_k.bias\n",
      "transformer_block.2.multi_head_att_layer.proj_v.weight\n",
      "transformer_block.2.multi_head_att_layer.proj_v.bias\n",
      "transformer_block.2.multi_head_att_layer.proj_linear.weight\n",
      "transformer_block.2.multi_head_att_layer.proj_linear.bias\n",
      "transformer_block.2.ff_layer.ff_layer.0.weight\n",
      "transformer_block.2.ff_layer.ff_layer.0.bias\n",
      "transformer_block.2.ff_layer.ff_layer.2.weight\n",
      "transformer_block.2.ff_layer.ff_layer.2.bias\n",
      "transformer_block.2.layer_norm_pre_multi_head.weight\n",
      "transformer_block.2.layer_norm_pre_multi_head.bias\n",
      "transformer_block.2.layer_norm_pre_feedforward.weight\n",
      "transformer_block.2.layer_norm_pre_feedforward.bias\n",
      "transformer_block.3.multi_head_att_layer.mask\n",
      "transformer_block.3.multi_head_att_layer.proj_q.weight\n",
      "transformer_block.3.multi_head_att_layer.proj_q.bias\n",
      "transformer_block.3.multi_head_att_layer.proj_k.weight\n",
      "transformer_block.3.multi_head_att_layer.proj_k.bias\n",
      "transformer_block.3.multi_head_att_layer.proj_v.weight\n",
      "transformer_block.3.multi_head_att_layer.proj_v.bias\n",
      "transformer_block.3.multi_head_att_layer.proj_linear.weight\n",
      "transformer_block.3.multi_head_att_layer.proj_linear.bias\n",
      "transformer_block.3.ff_layer.ff_layer.0.weight\n",
      "transformer_block.3.ff_layer.ff_layer.0.bias\n",
      "transformer_block.3.ff_layer.ff_layer.2.weight\n",
      "transformer_block.3.ff_layer.ff_layer.2.bias\n",
      "transformer_block.3.layer_norm_pre_multi_head.weight\n",
      "transformer_block.3.layer_norm_pre_multi_head.bias\n",
      "transformer_block.3.layer_norm_pre_feedforward.weight\n",
      "transformer_block.3.layer_norm_pre_feedforward.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n",
      "pred_head.weight\n",
      "pred_head.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict(): print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_to_custom_dict = {\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_q.weight\"      : \"vit.encoder.layer.0.attention.attention.query.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_q.bias\"        : \"vit.encoder.layer.0.attention.attention.query.bias\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_k.weight\"      : \"vit.encoder.layer.0.attention.attention.key.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_k.bias\"        : \"vit.encoder.layer.0.attention.attention.key.bias\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_v.weight\"      : \"vit.encoder.layer.0.attention.attention.value.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_v.bias\"        : \"vit.encoder.layer.0.attention.attention.value.bias\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_linear.weight\" : \"vit.encoder.layer.0.attention.output.dense.weight\",\n",
    "    \"transformer_block.0.multi_head_att_layer.proj_linear.bias\"   : \"vit.encoder.layer.0.attention.output.dense.bias\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.0.weight\"              : \"vit.encoder.layer.0.intermediate.dense.weight\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.0.bias\"                : \"vit.encoder.layer.0.intermediate.dense.bias\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.2.weight\"              : \"vit.encoder.layer.0.output.dense.weight\",\n",
    "    \"transformer_block.0.ff_layer.ff_layer.2.bias\"                : \"vit.encoder.layer.0.output.dense.bias\",\n",
    "    \"transformer_block.0.layer_norm_pre_multi_head.weight\"        : \"vit.encoder.layer.0.layernorm_before.weight\",\n",
    "    \"transformer_block.0.layer_norm_pre_multi_head.bias\"          : \"vit.encoder.layer.0.layernorm_before.bias\",\n",
    "    \"transformer_block.0.layer_norm_pre_feedforward.weight\"       : \"vit.encoder.layer.0.layernorm_after.weight\",\n",
    "    \"transformer_block.0.layer_norm_pre_feedforward.bias\"         : \"vit.encoder.layer.0.layernorm_after.bias\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_q.weight\"      : \"vit.encoder.layer.1.attention.attention.query.weight\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_q.bias\"        : \"vit.encoder.layer.1.attention.attention.query.bias\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_k.weight\"      : \"vit.encoder.layer.1.attention.attention.key.weight\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_k.bias\"        : \"vit.encoder.layer.1.attention.attention.key.bias\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_v.weight\"      : \"vit.encoder.layer.1.attention.attention.value.weight\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_v.bias\"        : \"vit.encoder.layer.1.attention.attention.value.bias\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_linear.weight\" : \"vit.encoder.layer.1.attention.output.dense.weight\",\n",
    "    \"transformer_block.1.multi_head_att_layer.proj_linear.bias\"   : \"vit.encoder.layer.1.attention.output.dense.bias\",\n",
    "    \"transformer_block.1.ff_layer.ff_layer.0.weight\"              : \"vit.encoder.layer.1.intermediate.dense.weight\",\n",
    "    \"transformer_block.1.ff_layer.ff_layer.0.bias\"                : \"vit.encoder.layer.1.intermediate.dense.bias\",\n",
    "    \"transformer_block.1.ff_layer.ff_layer.2.weight\"              : \"vit.encoder.layer.1.output.dense.weight\",\n",
    "    \"transformer_block.1.ff_layer.ff_layer.2.bias\"                : \"vit.encoder.layer.1.output.dense.bias\",\n",
    "    \"transformer_block.1.layer_norm_pre_multi_head.weight\"        : \"vit.encoder.layer.1.layernorm_before.weight\",\n",
    "    \"transformer_block.1.layer_norm_pre_multi_head.bias\"          : \"vit.encoder.layer.1.layernorm_before.bias\",\n",
    "    \"transformer_block.1.layer_norm_pre_feedforward.weight\"       : \"vit.encoder.layer.1.layernorm_after.weight\",\n",
    "    \"transformer_block.1.layer_norm_pre_feedforward.bias\"         : \"vit.encoder.layer.1.layernorm_after.bias\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_q.weight\"      : \"vit.encoder.layer.2.attention.attention.query.weight\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_q.bias\"        : \"vit.encoder.layer.2.attention.attention.query.bias\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_k.weight\"      : \"vit.encoder.layer.2.attention.attention.key.weight\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_k.bias\"        : \"vit.encoder.layer.2.attention.attention.key.bias\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_v.weight\"      : \"vit.encoder.layer.2.attention.attention.value.weight\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_v.bias\"        : \"vit.encoder.layer.2.attention.attention.value.bias\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_linear.weight\" : \"vit.encoder.layer.2.attention.output.dense.weight\",\n",
    "    \"transformer_block.2.multi_head_att_layer.proj_linear.bias\"   : \"vit.encoder.layer.2.attention.output.dense.bias\",\n",
    "    \"transformer_block.2.ff_layer.ff_layer.0.weight\"              : \"vit.encoder.layer.2.intermediate.dense.weight\",\n",
    "    \"transformer_block.2.ff_layer.ff_layer.0.bias\"                : \"vit.encoder.layer.2.intermediate.dense.bias\",\n",
    "    \"transformer_block.2.ff_layer.ff_layer.2.weight\"              : \"vit.encoder.layer.2.output.dense.weight\",\n",
    "    \"transformer_block.2.ff_layer.ff_layer.2.bias\"                : \"vit.encoder.layer.2.output.dense.bias\",\n",
    "    \"transformer_block.2.layer_norm_pre_multi_head.weight\"        : \"vit.encoder.layer.2.layernorm_before.weight\",\n",
    "    \"transformer_block.2.layer_norm_pre_multi_head.bias\"          : \"vit.encoder.layer.2.layernorm_before.bias\",\n",
    "    \"transformer_block.2.layer_norm_pre_feedforward.weight\"       : \"vit.encoder.layer.2.layernorm_after.weight\",\n",
    "    \"transformer_block.2.layer_norm_pre_feedforward.bias\"         : \"vit.encoder.layer.2.layernorm_after.bias\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_q.weight\"      : \"vit.encoder.layer.3.attention.attention.query.weight\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_q.bias\"        : \"vit.encoder.layer.3.attention.attention.query.bias\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_k.weight\"      : \"vit.encoder.layer.3.attention.attention.key.weight\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_k.bias\"        : \"vit.encoder.layer.3.attention.attention.key.bias\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_v.weight\"      : \"vit.encoder.layer.3.attention.attention.value.weight\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_v.bias\"        : \"vit.encoder.layer.3.attention.attention.value.bias\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_linear.weight\" : \"vit.encoder.layer.3.attention.output.dense.weight\",\n",
    "    \"transformer_block.3.multi_head_att_layer.proj_linear.bias\"   : \"vit.encoder.layer.3.attention.output.dense.bias\",\n",
    "    \"transformer_block.3.ff_layer.ff_layer.0.weight\"              : \"vit.encoder.layer.3.intermediate.dense.weight\",\n",
    "    \"transformer_block.3.ff_layer.ff_layer.0.bias\"                : \"vit.encoder.layer.3.intermediate.dense.bias\",\n",
    "    \"transformer_block.3.ff_layer.ff_layer.2.weight\"              : \"vit.encoder.layer.3.output.dense.weight\",\n",
    "    \"transformer_block.3.ff_layer.ff_layer.2.bias\"                : \"vit.encoder.layer.3.output.dense.bias\",\n",
    "    \"transformer_block.3.layer_norm_pre_multi_head.weight\"        : \"vit.encoder.layer.3.layernorm_before.weight\",\n",
    "    \"transformer_block.3.layer_norm_pre_multi_head.bias\"          : \"vit.encoder.layer.3.layernorm_before.bias\",\n",
    "    \"transformer_block.3.layer_norm_pre_feedforward.weight\"       : \"vit.encoder.layer.3.layernorm_after.weight\",\n",
    "    \"transformer_block.3.layer_norm_pre_feedforward.bias\"         : \"vit.encoder.layer.3.layernorm_after.bias\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state_dict = model.state_dict()\n",
    "for k in encoder_state_dict.keys():\n",
    "    if not k in google_to_custom_dict: continue\n",
    "    k_google = google_to_custom_dict[k]\n",
    "    encoder_state_dict[k] = model_weight_dict[k_google]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peaknet-1.0",
   "language": "python",
   "name": "peaknet-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
